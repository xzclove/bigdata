Flume监听Nginx日志信息，写入到hdfs
	1. 编写nginx配置信息  nginx.conf
		在nginx根目录  /usr/local/nginx 下创建 www/source文件夹，并将BfImg图片移动到该目录下。		
	2. 编写flume的agent配置信息
	       test2.conf
	3. 移动hdfs依赖包到flume的lib文件夹中。
	  
	  cp /opt/modules/hadoop/hadoop-2.5.0/share/hadoop/common/lib/commons-configuration-1.6.jar /opt/modules/flume-1.5.0/lib
	  cp /opt/modules/hadoop/hadoop-2.5.0/share/hadoop/common/lib/hadoop-auth-2.5.0.jar /opt/modules/flume-1.5.0/lib
	  cp /opt/modules/hadoop/hadoop-2.5.0/share/hadoop/common/hadoop-common-2.5.0.jar /opt/modules/flume-1.5.0/lib
	  cp /opt/modules/hadoop/hadoop-2.5.0/share/hadoop/hdfs/hadoop-hdfs-2.5.0.jar /opt/modules/flume-1.5.0/lib
	  
	4. 进入flume根目录，启动flume: flume-ng agent --conf ./conf/ --conf-file ./conf/test2.conf --name agent
	5. 发送http请求查看是否成功。
	
	
	问题：flume突然挂掉，hdfs中产生临时文件，跑mapreduce程序读该文件的时候，可能会出现异常？
		解决办法：
			1. 现copy文件到其他目录
				hdfs dfs -cp /logs/11/17/BF-09.1447769674855.log.tmp /BF-09.1447769674855.log.tmp
			2. 删除现有的文件
				hdfs dfs -rm /logs/11/17/BF-09.1447769674855.log.tmp
			3. 将copy的文件复制回去。
				hdfs dfs -cp /BF-09.1447769674855.log.tmp /logs/11/17/BF-09.1447769674855.log